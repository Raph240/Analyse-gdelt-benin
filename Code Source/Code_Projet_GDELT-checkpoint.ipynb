{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70feb77a-274e-45f4-abcc-775ebfe03675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import docx\n",
    "\n",
    "# Fonction pour extraire le texte d'un fichier DOCX\n",
    "def extract_text_from_docx(doc_path):\n",
    "    doc = docx.Document(doc_path)\n",
    "    full_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return full_text\n",
    "\n",
    "# Fonction pour extraire les liens\n",
    "def extract_links(text):\n",
    "    url_pattern = r\"https?://[^\\s]+\"  # Expression r√©guli√®re pour d√©tecter les liens\n",
    "    links = re.findall(url_pattern, text)\n",
    "    return links\n",
    "\n",
    "# Chemin vers le document WORD ( Base de donn√©es isheero.docx )\n",
    "doc_path = \"Base de donn√©es isheero.docx\"\n",
    "\n",
    "# Extraction du texte et des liens\n",
    "text = extract_text_from_docx(doc_path)\n",
    "links = extract_links(text)\n",
    "\n",
    "# Nature de links\n",
    "import os\n",
    "\n",
    "file_path = \"links\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"‚ùå Le fichier 'links' n'existe pas.\")\n",
    "elif os.path.isdir(file_path):\n",
    "    print(\"‚ùå 'links' est un dossier, pas un fichier.\")\n",
    "else:\n",
    "    print(\"‚úÖ 'links' est bien un fichier.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55360c8d-9845-4795-93d7-fcd8869b30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Dossier \"links\" ( dans le m√™me r√©pertoire que le script ou notebook )\n",
    "folder_name = \"links\"\n",
    "\n",
    "# Obtenir le chemin absolu\n",
    "abs_path = os.path.abspath(folder_name)\n",
    "\n",
    "# Afficher le chemin\n",
    "print(f\" Chemin absolu du dossier 'links' :\\n{abs_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ceb232-f9fd-42a9-8c7b-749f534ad0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce code sert √† √©crire les liens extraits dans des fichiers √† l‚Äôint√©rieur du dossier links\n",
    "\n",
    "with open(\"links/mes_liens.txt\", \"w\") as f:\n",
    "    for link in links:\n",
    "        f.write(link + \"\\n\")\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bb8c7-5f67-4118-b66a-fbe128ca05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Chemin vers le dossier \"links\"\n",
    "dossier_links = \"Links\"\n",
    "\n",
    "# Liste pour stocker tous les liens\n",
    "tous_liens = []\n",
    "\n",
    "# V√©rifie si le dossier existe\n",
    "if not os.path.exists(dossier_links):\n",
    "    print(f\"‚ùå Le dossier n'existe pas : {dossier_links}\")\n",
    "elif os.path.isfile(dossier_links):\n",
    "    print(\"C'est un fichier.\")\n",
    "elif not os.path.isdir(dossier_links):\n",
    "    print(f\"‚ùå Le chemin n'est pas un dossier : {dossier_links}\")\n",
    "else:\n",
    "    print(f\" Dossier trouv√© : {dossier_links}\")\n",
    "    print(\" Lecture des fichiers...\\n\")\n",
    "\n",
    "    # Parcourt tous les fichiers dans le dossier (  os.listdir() sert √† lister le contenu d‚Äôun dossier mais pas celui d‚Äôun fichier texte. )\n",
    "    for nom_fichier in os.listdir(dossier_links):\n",
    "        chemin_fichier = os.path.join(dossier_links, nom_fichier)\n",
    "\n",
    "        # V√©rifie que c'est bien un fichier texte\n",
    "        if os.path.isfile(chemin_fichier) and nom_fichier.endswith(\".txt\"):\n",
    "            print(f\"üìù Contenu de : {nom_fichier}\")\n",
    "            with open(chemin_fichier, \"r\", encoding=\"utf-8\") as f:\n",
    "                \n",
    "                lignes = f.readlines()\n",
    "                # Nettoyage de chaque ligne (suppression \\n et espaces)\n",
    "                liens_propres = [ligne.strip() for ligne in lignes if ligne.strip()]\n",
    "                tous_liens.extend(liens_propres)\n",
    "\n",
    "# R√©sultat\n",
    "print(\"‚úÖ Tous les liens extraits :\")\n",
    "print(tous_liens)\n",
    "\n",
    "print(f\" Nombre total de liens : {len(tous_liens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af66243-36bd-4bbd-a3ae-1bf01e8fc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aper√ßu des 10 premiers liens :\")\n",
    "print(tous_liens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08443303-8f81-4cc4-b635-8ee22f47fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection des liens du premier trimestre de l'ann√©e 2025\n",
    "tous_liens_1 = tous_liens [ 89922 : 104476 ]\n",
    "tous_liens_2 = tous_liens [ 552 : 11764 ]\n",
    "tous_liens_3 = tous_liens_1 + tous_liens_2\n",
    "\n",
    "# Traitement\n",
    "URLS = []\n",
    "\n",
    "# Ann√©e de remplacement\n",
    "nouvelle_annee = \"2025\"\n",
    "\n",
    "for url in  tous_liens_3:\n",
    "#    partie_avant, reste =  tous_liens_1.split(\"gdeltv2/\", 1)   # coupe 1 seule fois \n",
    "    partie_avant, reste = url.split(\"gdeltv2/\", 1)  # ici c‚Äôest bien sur chaque URL. On n'applique pas .split sur une liste mais seulement sur les chaines de caract√®res\n",
    "    \n",
    "    nouvelle_date = nouvelle_annee + reste[4:]  # remplacer les 4 premiers chiffres\n",
    "    nouvelle_url = partie_avant + \"gdeltv2/\" + nouvelle_date\n",
    "    URLS.append(nouvelle_url)\n",
    "\n",
    "# Affichage\n",
    "for u in URLS :\n",
    "    print ( u )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d1021-8b36-4011-b242-c84638dc2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### ce code sert √† convertir la liste URLS en dataframe pour un usage ult√©rieur\n",
    "df_liens = pd.DataFrame( URLS, columns= ['Liens'] )\n",
    "\n",
    "### pour transformer le dataframe en csv\n",
    "df_liens.to_csv('Liens_2025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bf5da-f139-4c64-8a47-f219419fd65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la s√©lection des fichiers events\n",
    "LINKS = [url for url in URLS if url.endswith(\"export.CSV.zip\")]\n",
    "\n",
    "for u in LINKS :\n",
    "    print(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a05b9d-3493-46ae-af3f-1def1ab060a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "OUTPUT_DIR = 'GDELT_DATA'    # R√©pertoire de sortie pour les fichiers t√©l√©charg√©s\n",
    "\n",
    "\n",
    "# Ce code sert √† t√©l√©charger et extraire les fichiers events pour une p√©riode donn√©e\n",
    "def download_and_extract(links, output_dir):\n",
    "    for link in tqdm(links, desc=\"T√©l√©chargement des fichiers GDELT\"):\n",
    "        file_name = link.split(\"/\")[-1]\n",
    "\n",
    "        # V√©rifier si le fichier existe d√©j√† pour √©viter de le ret√©l√©charger\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(link, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            else:\n",
    "                print(f\"Erreur de t√©l√©chargement pour {link}\")\n",
    "                continue\n",
    "\n",
    "        # D√©compression du fichier ZIP\n",
    "        # V√©rifier si c'est bien un fichier ZIP\n",
    "        if zipfile.is_zipfile(file_name):\n",
    "            print(\"Le fichier est un ZIP valide.\")\n",
    "            with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(output_dir)\n",
    "                print(\"Extraction r√©ussie !\")\n",
    "        else:\n",
    "            print(\"Erreur : Le fichier n'est pas un ZIP valide.\")\n",
    "            continue \n",
    "            \n",
    "        os.remove(file_name)  # Supprimer le fichier ZIP apr√®s extraction pour √©conomiser l'espace\n",
    "\n",
    "download_and_extract( LINKS , OUTPUT_DIR)\n",
    "\n",
    "print ( OUTPUT_DIR )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78412eb-ce36-4968-8f76-8fbf3c9bc24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_DIR = 'GDELT_DATA' \n",
    "COUNTRY_CODE = \"BN\"\n",
    "\n",
    "\n",
    "#  Filtrer les donn√©es par pays\n",
    "def filter_events_by_country(output_dir, country_code):\n",
    "    all_data = []\n",
    "    for file in tqdm(os.listdir(output_dir), desc=\"Filtrage des fichiers CSV\"):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        try:\n",
    "            # Lecture des donn√©es CSV en utilisant des chunks pour traiter des fichiers volumineux\n",
    "            for chunk in pd.read_csv(file_path, sep='\\t', chunksize=100000, header=None, encoding='latin1', dtype=str):\n",
    "                # La colonne 53 correspond aux √©v√®nements du B√©nin \n",
    "                filtered_chunk = chunk[chunk[53] == country_code]\n",
    "                all_data.append(filtered_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture de {file}: {e}\")\n",
    "    # Combiner toutes les donn√©es filtr√©es en un seul DataFrame\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "benin_data = filter_events_by_country(OUTPUT_DIR, COUNTRY_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538baf2-4e9e-4402-8ecd-ae444812f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATASET = 'Benin_dataset.csv'  # Nom du fichier final\n",
    "\n",
    "\n",
    "# √âtape 5 : enregistrer le dataset final en csv\n",
    "def save_final_dataset(dataframe, output_file):\n",
    "    dataframe.to_csv(output_file, index=False)\n",
    "    print(f\" Dataset final enregistr√© sous {output_file}\")\n",
    "\n",
    "save_final_dataset(benin_data, FINAL_DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586b566-a2e2-4b34-9055-cc9327d4e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv( 'Benin_dataset.csv' , sep = ',' , encoding = 'latin1' , dtype = 'str' )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5a9c7-f220-4ad0-b7bd-136fde23d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame vide avec 10 colonnes pour la structuration finale \n",
    "df_nouveau = pd.DataFrame(columns=[\n",
    "     'Ann√©e', 'Mois' , 'Date_Heure', 'Pays', 'Code_Pays', \n",
    "    'Adresse_Web', 'Source', 'Tendance', 'R√©sum√©', 'Sentiment_Global'\n",
    "])\n",
    "\n",
    "# Remplir certaines colonnes du nouveau DataFrame avec les valeurs de df\n",
    "df_nouveau['Ann√©e'] = df.iloc[ : , 3]\n",
    "df_nouveau['Date_Heure'] = df.iloc[ : , 59]\n",
    "df_nouveau['Pays'] = df.iloc[ : , 52]\n",
    "df_nouveau['Code_Pays'] = df.iloc[ : , 53]\n",
    "df_nouveau['Adresse_Web'] = df.iloc[ : , 60]\n",
    "df_nouveau['Source'] = 'GDELT'\n",
    "\n",
    "df_nouveau.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79d003-ac77-4ac7-b7ca-966e5c13ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Liste vide pour stocker les mois\n",
    "mois_list = []\n",
    "\n",
    "# Boucle sur chaque ligne de df1\n",
    "for code in df.iloc[ : , 2]:\n",
    "    mois_code = str(code)[-2:]  # On prend les 2 derniers caract√®res\n",
    "    if mois_code == \"01\":\n",
    "        mois_list.append(\"Janvier\")\n",
    "    elif mois_code == \"02\":\n",
    "        mois_list.append(\"F√©vrier\")\n",
    "    elif mois_code == \"03\":\n",
    "        mois_list.append(\"Mars\")\n",
    "    elif mois_code == \"04\":\n",
    "        mois_list.append(\"Avril\")\n",
    "    elif mois_code == \"05\":\n",
    "        mois_list.append(\"Mai\")\n",
    "    elif mois_code == \"06\":\n",
    "        mois_list.append(\"Juin\")\n",
    "    elif mois_code == \"07\":\n",
    "        mois_list.append(\"Juillet\")\n",
    "    elif mois_code == \"08\":\n",
    "        mois_list.append(\"Ao√ªt\")\n",
    "    elif mois_code == \"09\":\n",
    "        mois_list.append(\"Septembre\")\n",
    "    elif mois_code == \"10\":\n",
    "        mois_list.append(\"Octobre\")\n",
    "    elif mois_code == \"11\":\n",
    "        mois_list.append(\"Novembre\")\n",
    "    elif mois_code == \"12\":\n",
    "        mois_list.append(\"D√©cembre\")\n",
    "    else:\n",
    "        mois_list.append(\"Inconnu\")\n",
    "\n",
    "# Cr√©ation du deuxi√®me DataFrame\n",
    "df_nouveau ['Mois'] = pd.DataFrame({\"Mois\": mois_list})\n",
    "\n",
    "# R√©sultat\n",
    "df_nouveau.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d662bf-bb84-4cf0-8c67-af74ee29fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ce code sert √† t√©l√©charger les articles contenus dans chaque liens inscrit dans la derni√®re colonne de df en ordre et avec une gestion d'erreur\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from newspaper import Article\n",
    "\n",
    "# Dossier de sortie\n",
    "OUTPUT_FOLDER = \"Publications_file\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df = pd.read_csv('Benin_dataset.csv', sep=',', encoding='latin1', dtype='str', header=None)\n",
    "total_articles = len(df)\n",
    "\n",
    "# Initialiser les logs\n",
    "logs = []\n",
    "\n",
    "# Boucle principale\n",
    "for idx, row in df.iterrows():\n",
    "    url = row.iloc[-1]\n",
    "    article_num = str(idx + 1).zfill(len(str(total_articles)))  # ex: '001', '045', ...\n",
    "\n",
    "    try:\n",
    "        article = Article(url, language='fr')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "\n",
    "        if len(text.strip()) > 0:\n",
    "            filename = f\"{article_num}_article.txt\"\n",
    "            filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "            status = \"OK\"\n",
    "            print(f\"[‚úî] Article {article_num} t√©l√©charg√©.\")\n",
    "        else:\n",
    "            filename = f\"{article_num}_Article_vide.txt\"\n",
    "            filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\")\n",
    "            status = \"Vide\"\n",
    "            print(f\"[‚úò] Article vide : {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        filename = f\"{article_num}_{error_type}.txt\"\n",
    "        filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")\n",
    "        status = f\"Erreur ({error_type})\"\n",
    "        print(f\"[!] Erreur avec {url} : {e}\")\n",
    "\n",
    "    # Ajouter une ligne au log\n",
    "    logs.append({\n",
    "        \"Num√©ro\": article_num,\n",
    "        \"URL\": url,\n",
    "        \"Statut\": status,\n",
    "        \"Fichier\": filename\n",
    "    })\n",
    "\n",
    "# Sauvegarder le fichier log √† la fin\n",
    "log_df = pd.DataFrame(logs)\n",
    "log_df.to_csv(os.path.join(OUTPUT_FOLDER, \"log.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n Journal de traitement enregistr√© dans : log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c19590-ff4e-459d-b21d-993607643cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ce code permet d'identifier les tendances et les r√©sum√©s de tous les articles contenus dans tous les liens de df sauf le premier liens\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "# Dossier contenant les fichiers texte\n",
    "output_folder = \"Publications_file\"\n",
    "\n",
    "# Liste des r√©sultats\n",
    "r√©sum = []\n",
    "\n",
    "# Dictionnaire de cat√©gories th√©matiques\n",
    "\n",
    "thematic_categories = {\n",
    "    \"√©motionnel\": [\"col√®re\", \"peur\", \"espoir\", \"frustration\", \"solidarit√©\", \"joie\", \"tristesse\"],\n",
    "    \"√©ducation\": [\"√©tudiant\", \"√©l√®ve\", \"universit√©\", \"√©cole\", \"enseignant\", \"examen\", \"formation\", \"apprentissage\", \"cours\", \"scolarit√©\", \"dipl√¥me\", \"bourse\", \"alphab√©tisation\", \"enseignement sup√©rieur\"],\n",
    "    \"culture\": [\"culture\", \"coutume\", \"tradition\", \"musique\", \"peinture\", \"danse\", \"festival\", \"patrimoine\", \"art\", \"exposition\", \"sculpture\", \"cr√©ation artistique\"],\n",
    "    \"politique\": [\"gouvernement\", \"√©lection\", \"vote\", \"pr√©sident\", \"ministre\", \"d√©put√©\", \"r√©forme\", \"loi\", \"r√©publique\", \"constitution\", \"campagne\", \"parti politique\"],\n",
    "    \"guerre\": [\"conflit\", \"bataille\", \"militaire\", \"arm√©e\", \"arme\", \"attaque\", \"bombardement\", \"soldat\", \"insurrection\", \"r√©bellion\", \"invasion\", \"violence arm√©e\"],\n",
    "    \"paix\": [\"paix\", \"cessez-le-feu\", \"r√©conciliation\", \"dialogue\", \"m√©diation\", \"accord\", \"cohabitation\", \"diplomatie\"],\n",
    "    \"f√™te\": [\"f√™te\", \"anniversaire\", \"c√©l√©bration\", \"carnaval\", \"mariage\", \"comm√©moration\", \"√©v√©nement festif\", \"r√©jouissance\"],\n",
    "    \"d√©c√®s\": [\"mort\", \"d√©c√®s\", \"obs√®ques\", \"inhumation\", \"fun√©railles\", \"disparition\", \"deuil\"],\n",
    "    \"corruption\": [\"corruption\", \"fraude\", \"d√©tournement\", \"scandale\", \"enqu√™te\", \"abus de pouvoir\", \"transparence\", \"client√©lisme\"],\n",
    "    \"science\": [\"recherche\", \"exp√©rience\", \"laboratoire\", \"innovation\", \"scientifique\", \"th√©orie\", \"d√©couverte\"],\n",
    "    \"litt√©rature\": [\"livre\", \"roman\", \"√©crivain\", \"auteur\", \"po√©sie\", \"lecture\", \"biblioth√®que\", \"publication\"],\n",
    "    \"emploi\": [\"emploi\", \"ch√¥mage\", \"recrutement\", \"stage\", \"travail\", \"carri√®re\", \"emploi jeune\", \"offre d‚Äôemploi\"],\n",
    "    \"bourse\": [\"bourse\", \"aide financi√®re\", \"soutien √©tudiant\", \"bourse d‚Äô√©tude\", \"financement\", \"subvention\"],\n",
    "    \"technologie\": [\"technologie\" , \"num√©rique\", \"internet\", \"logiciel\", \"application\", \"r√©seaux sociaux\", \"intelligence artificielle\", \"hackathon\", \"robotique\", \"cybers√©curit√©\"],\n",
    "    \"transport\": [\"route\", \"v√©hicule\", \"trafic\", \"bus\", \"train\", \"accident\", \"circulation\", \"a√©roport\", \"transport en commun\", \"infrastructure\"],\n",
    "    \"justice\": [\"tribunal\", \"juge\", \"proc√®s\", \"plainte\", \"avocat\", \"peine\", \"infraction\", \"mandat\", \"condamnation\"],\n",
    "    \"sant√©\": [\"h√¥pital\", \"maladie\", \"sant√©\", \"vaccin\", \"soins\", \"pand√©mie\", \"√©pid√©mie\", \"urgence\", \"chirurgie\", \"m√©dicament\"],\n",
    "    \"environnement\": [\"climat\", \"√©cologie\", \"biodiversit√©\", \"pollution\", \"r√©chauffement\", \"for√™t\", \"durabilit√©\", \"recyclage\", \"catastrophe naturelle\"],\n",
    "    \"m√©dias\": [\"presse\", \"journaliste\", \"m√©dia\", \"reportage\", \"information\", \"√©mission\", \"radio\", \"t√©l√©vision\", \"libert√© de la presse\"],\n",
    "    \"religion\": [\"religion\", \"√©glise\", \"mosqu√©e\", \"temple\", \"culte\", \"foi\", \"pri√®re\", \"f√™te religieuse\", \"chr√©tien\", \"musulman\", \"imam\", \"bible\", \"coran\"],\n",
    "    \"migration\": [\"migrant\", \"r√©fugi√©\", \"asile\", \"immigration\", \"diaspora\", \"exil\", \"fronti√®re\", \"int√©gration\"],\n",
    "    \"s√©curit√©\": [\"police\", \"criminalit√©\", \"s√©curit√©\", \"violence\", \"vol\", \"braquage\", \"terrorisme\", \"drogue\", \"garde\", \"milice\"],\n",
    "    \"genre\": [\"femme\", \"√©galit√©\", \"f√©minisme\", \"harc√®lement\", \"discrimination\", \"droits des femmes\", \"sexisme\", \"violence conjugale\", \"genre\"],\n",
    "    \"jeunesse\": [\"jeunesse\", \"jeune\", \"talent\", \"espoir\", \"ambition\", \"futur\", \"engagement jeune\", \"g√©n√©ration\"],\n",
    "    \"sport\": [\"sport\", \"football\", \"match\", \"comp√©tition\", \"athl√®te\", \"champion\", \"entra√Æneur\", \"tournoi\", \"ligue\", \"Jeux Olympiques\"],\n",
    "    \"urbanisme\": [\"ville\", \"quartier\", \"urbanisation\", \"infrastructure\", \"logement\", \"architecture\", \"voirie\"],\n",
    "    \"agriculture\": [\"agriculture\", \"ferme\", \"r√©colte\", \"semence\", \"champ\", \"√©levage\", \"engrais\", \"paysan\", \"agroalimentaire\"],\n",
    "    \"droits_humains\": [\"droits\", \"libert√©\", \"justice\", \"oppression\", \"militant\", \"r√©pression\", \"torture\", \"droit international\", \"ONG\"],\n",
    "    \"diplomatie\": [\"coop√©ration\", \"relation internationale\", \"accord bilat√©ral\", \"visite officielle\", \"ambassade\", \"n√©gociation\", \"partenariat\"],\n",
    "    \"vie_quotidienne\": [\"famille\", \"foyer\", \"march√©\", \"eau\", \"√©lectricit√©\", \"logement\", \"alimentation\", \"prix\", \"consommation\", \"vie ch√®re\"],\n",
    "    \"√©nergie\": [\"√©nergie\", \"carburant\", \"gaz\", \"p√©trole\", \"√©lectricit√©\", \"solaire\", \"nucl√©aire\", \"hydro√©lectrique\", \"transition √©nerg√©tique\"],\n",
    "    \"tourisme\": [\"tourisme\", \"site touristique\", \"voyage\", \"visite\", \"patrimoine\", \"h√¥tel\", \"guide touristique\"],\n",
    "    \"innovation\": [\"startup\", \"technopole\", \"incubateur\", \"technologie\", \"id√©e\", \"cr√©ativit√©\", \"prototype\", \"pitch\"],\n",
    "    \"√©galit√© sociale\": [\"justice sociale\", \"in√©galit√©\", \"solidarit√©\", \"exclusion\", \"inclusion\", \"pauvret√©\", \"minorit√©s\"],\n",
    "    \"hostilit√©\": [\"haine\", \"violence\", \"menace\", \"intimidation\", \"insulte\", \"confrontation\", \"tension\"]\n",
    "}\n",
    "\n",
    "# Fonctions\n",
    "def preprocess_and_segment(text):\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    doc = nlp(text.lower())\n",
    "    sentences = list(doc.sents)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.pos_ == \"NOUN\" and token.is_alpha]\n",
    "    return tokens, sentences, doc\n",
    "\n",
    "def detect_topics(tokens):\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    lda_model = LdaModel(corpus, num_topics=1, id2word=dictionary, passes=10)\n",
    "    return lda_model, dictionary\n",
    "\n",
    "def generate_narrative_summary(text, num_summary_sentences=1):       # mieux vaut passer √† 1 phrase\n",
    "    tokens, sentences, doc = preprocess_and_segment(text)\n",
    "    lda_model, dictionary = detect_topics(tokens)\n",
    "\n",
    "    topic_words = []\n",
    "    for idx, topic in lda_model.show_topics(formatted=False, num_words=10):\n",
    "        topic_words.extend([word for word, _ in topic])\n",
    "\n",
    "    sentence_scores = defaultdict(int)\n",
    "    for sent in sentences:\n",
    "        for word in topic_words:\n",
    "            if word in sent.text.lower():\n",
    "                sentence_scores[sent.text] += 1\n",
    "\n",
    "    best_sentences = heapq.nlargest(num_summary_sentences, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    matched_categories = []\n",
    "    for category, ref_words in thematic_categories.items():\n",
    "        if any(word in ref_words for word in topic_words):\n",
    "            matched_categories.append(category)\n",
    "\n",
    "    summary_text = \" \".join(best_sentences).strip()\n",
    "    themes_text = \", \".join(set(matched_categories)) if matched_categories else \"Sujets g√©n√©raux ou divers\"\n",
    "\n",
    "    return themes_text, summary_text\n",
    "\n",
    "# Extraire le num√©ro au d√©but du nom de fichier\n",
    "def extract_article_number(filename):\n",
    "    match = re.match(r\"(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else float(\"inf\")\n",
    "\n",
    "# Lister et trier les fichiers texte\n",
    "file_list = sorted(\n",
    "    [f for f in os.listdir(output_folder) if f.endswith(\".txt\")],\n",
    "    key=extract_article_number\n",
    ")\n",
    "\n",
    "# Parcourir les fichiers\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(output_folder, filename)\n",
    "    article_number = extract_article_number(filename)\n",
    "\n",
    "    # Cas d‚Äôarticle vide ou fichier d‚Äôerreur\n",
    "    if \"Article_vide\" in filename or any(err in filename for err in [\"Error\", \"Exception\", \"FileNotFound\", \"Timeout\"]):\n",
    "        theme = \"Inconnu\"\n",
    "        summary = \"Illisible\"\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "\n",
    "            if len(text.strip()) == 0:\n",
    "                theme = \"Inconnu\"\n",
    "                summary = \"Illisible\"\n",
    "            else:\n",
    "                theme, summary = generate_narrative_summary(text)\n",
    "\n",
    "        except Exception:\n",
    "            theme = \"Inconnu\"\n",
    "            summary = \"Illisible\"\n",
    "\n",
    "    # Ajouter au tableau\n",
    "    r√©sum.append({\n",
    "        \"Num√©ro d‚Äôarticle\": article_number,\n",
    "        \"Nom du fichier\": filename,\n",
    "        \"Tendance\": theme,\n",
    "        \"R√©sum√©\": summary\n",
    "    })\n",
    "\n",
    "# Cr√©er le DataFrame\n",
    "df_r√©sum = pd.DataFrame(r√©sum)\n",
    "\n",
    "# Affichage\n",
    "df_r√©sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a63aa4-4409-4688-9c8c-1284336e17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version modifi√©e du code pour qu‚Äôil traite un seul lien URL au lieu d‚Äôun dossier de fichiers texte ( uniquement le premier liens )\n",
    "\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from newspaper import Article\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Lien unique √† traiter\n",
    "url = \"https://kaloumpresse.com/2019/05/27/nouvelle-c\"\n",
    "\n",
    "# Chargement du mod√®le spaCy fran√ßais\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Dictionnaire des th√©matiques \n",
    "thematic_categories = {\n",
    "    \"politique\": [\"gouvernement\", \"√©lection\", \"pr√©sident\", \"ministre\", \"r√©forme\"],\n",
    "    \"sant√©\": [\"h√¥pital\", \"maladie\", \"vaccin\", \"soins\", \"pand√©mie\"],\n",
    "    \"√©ducation\": [\"√©cole\", \"universit√©\", \"√©tudiant\", \"enseignant\", \"formation\"],\n",
    "    \"justice\": [\"tribunal\", \"juge\", \"proc√®s\", \"plainte\", \"avocat\", \"peine\", \"infraction\", \"mandat\", \"condamnation\"],\n",
    "}\n",
    "\n",
    "# Fonctions\n",
    "def preprocess_and_segment(text):\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    doc = nlp(text.lower())\n",
    "    sentences = list(doc.sents)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.pos_ == \"NOUN\" and token.is_alpha]\n",
    "    return tokens, sentences, doc\n",
    "\n",
    "def detect_topics(tokens):\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    lda_model = LdaModel(corpus, num_topics=1, id2word=dictionary, passes=10)\n",
    "    return lda_model, dictionary\n",
    "\n",
    "def generate_narrative_summary(text, num_summary_sentences=1):       # mieux vaut passer √† 1 phrase\n",
    "    tokens, sentences, doc = preprocess_and_segment(text)\n",
    "    lda_model, dictionary = detect_topics(tokens)\n",
    "\n",
    "    topic_words = []\n",
    "    for idx, topic in lda_model.show_topics(formatted=False, num_words=10):\n",
    "        topic_words.extend([word for word, _ in topic])\n",
    "\n",
    "    sentence_scores = defaultdict(int)\n",
    "    for sent in sentences:\n",
    "        for word in topic_words:\n",
    "            if word in sent.text.lower():\n",
    "                sentence_scores[sent.text] += 1\n",
    "\n",
    "    best_sentences = heapq.nlargest(num_summary_sentences, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    matched_categories = []\n",
    "    for category, ref_words in thematic_categories.items():\n",
    "        if any(word in ref_words for word in topic_words):\n",
    "            matched_categories.append(category)\n",
    "\n",
    "    summary_text = \" \".join(best_sentences).strip()\n",
    "    themes_text = \", \".join(set(matched_categories)) if matched_categories else \"Sujets g√©n√©raux ou divers\"\n",
    "\n",
    "    return themes_text, summary_text\n",
    "\n",
    "\n",
    "# √âtape 1 : T√©l√©charger et parser l'article\n",
    "try:\n",
    "    article = Article(url, language='fr')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "\n",
    "    if len(text.strip()) == 0:\n",
    "        raise ValueError(\"Texte vide\")\n",
    "\n",
    "    # √âtape 2 : R√©sum√© + Th√®me\n",
    "    theme, summary = generate_narrative_summary(text)\n",
    "\n",
    "    # Affichage du r√©sultat\n",
    "    print(\"\\nüéØ R√©sultats analytiques pour l'URL :\")\n",
    "    print(f\"- Th√®me(s) dominant(s) : {theme}\")\n",
    "    print(f\"- R√©sum√© :\\n{summary}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Erreur] Impossible d'analyser l'article : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bddac2-8d17-4c04-b841-2e22228a3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pour compl√©ter le tableau\n",
    "df_r√©sum.iloc [0,2] = theme\n",
    "df_r√©sum.iloc [0,3] = summary\n",
    "\n",
    "df_r√©sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a6de6-ada6-4957-9d8a-fe635de40ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ce code permet d'identifier les sentiments exprim√©s dans tous les articles contenus dans tous les liens de df sauf le premier\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Dossier contenant les fichiers texte\n",
    "output_folder = \"Publications_file\"\n",
    "sentim = []\n",
    "\n",
    "# Chargement du mod√®le de sentiment multilingue\n",
    "sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Fonction d‚Äôanalyse du sentiment\n",
    "def analyze_overall_sentiment(text):\n",
    "    result = sentiment_analyzer(text[:512])[0]  # Tronquer √† 512 caract√®res max\n",
    "    label = result[\"label\"]\n",
    "    score = result[\"score\"]\n",
    "    stars = int(label[0])  # Exemple : '4 stars' ‚Üí 4\n",
    "    if stars <= 2:\n",
    "        sentiment = \"M√©content\"\n",
    "    elif stars == 3:\n",
    "        sentiment = \"Indiff√©rent\"\n",
    "    else:\n",
    "        sentiment = \"Content\"\n",
    "    return sentiment, round(score, 3)\n",
    "\n",
    "# Fonction d‚Äôinterpr√©tation du score de confiance\n",
    "def interpret_confiance(score):\n",
    "    if score >= 0.90:\n",
    "        return \"Tr√®s fiable\"\n",
    "    elif score >= 0.75:\n",
    "        return \"Fiable\"\n",
    "    elif score >= 0.60:\n",
    "        return \"Faible confiance\"\n",
    "    else:\n",
    "        return \"Peu fiable\"\n",
    "\n",
    "# Liste des fichiers texte tri√©s (ordre alphab√©tique)\n",
    "filenames = sorted([f for f in os.listdir(output_folder) if f.endswith(\".txt\")])\n",
    "\n",
    "# Analyse de chaque fichier\n",
    "for filename in filenames:\n",
    "    file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    # V√©rifier si le fichier est un article vide ou une erreur\n",
    "    if \"Article_vide\" in filename or \"ArticleException\" in filename:\n",
    "        sentiment = \"Inconnu\"\n",
    "        confiance = \"Inconnu\"\n",
    "        interpretation = \"Inconnu\"\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "\n",
    "            sentiment, confiance = analyze_overall_sentiment(text)\n",
    "            interpretation = interpret_confiance(confiance)\n",
    "\n",
    "        except Exception as e:\n",
    "            sentiment = \"Inconnu\"\n",
    "            confiance = \"Inconnu\"\n",
    "            interpretation = f\"Erreur: {type(e).__name__}\"\n",
    "\n",
    "    sentim.append({\n",
    "        \"Fichier\": filename,\n",
    "        \"Sentiment\": sentiment,\n",
    "        \"Confiance\": confiance,\n",
    "        \"Interpr√©tation\": interpretation\n",
    "    })\n",
    "\n",
    "# Construire le DataFrame final\n",
    "df_sentim = pd.DataFrame(sentim)\n",
    "\n",
    "# Afficher\n",
    "df_sentim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91534c1-75fb-4fa6-94ed-cd2f8d93e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version modifi√©e du code pour qu‚Äôil traite un seul lien URL au lieu d‚Äôun dossier de fichiers texte ( uniquement le premier liens )\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from newspaper import Article\n",
    "\n",
    "# ‚û§ Lien √† analyser\n",
    "url = \"https://kaloumpresse.com/2019/05/27/nouvelle-c\"\n",
    "\n",
    "# Chargement du mod√®le de sentiment multilingue\n",
    "sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Fonction d‚Äôanalyse du sentiment\n",
    "def analyze_overall_sentiment(text):\n",
    "    result = sentiment_analyzer(text[:512])[0]  # Tronquer √† 512 tokens max\n",
    "    label = result[\"label\"]  # ex : '4 stars'\n",
    "    score = result[\"score\"]\n",
    "    stars = int(label[0])\n",
    "    if stars <= 2:\n",
    "        sentiment = \"M√©content\"\n",
    "    elif stars == 3:\n",
    "        sentiment = \"Indiff√©rent\"\n",
    "    else:\n",
    "        sentiment = \"Content\"\n",
    "    return sentiment, round(score, 3)\n",
    "\n",
    "# Fonction d‚Äôinterpr√©tation du score de confiance\n",
    "def interpret_confiance(score):\n",
    "    if score >= 0.90:\n",
    "        return \"Tr√®s fiable\"\n",
    "    elif score >= 0.75:\n",
    "        return \"Fiable\"\n",
    "    elif score >= 0.60:\n",
    "        return \"Faible confiance\"\n",
    "    else:\n",
    "        return \"Peu fiable\"\n",
    "\n",
    "# Extraction de l‚Äôarticle depuis l‚ÄôURL\n",
    "try:\n",
    "    article = Article(url, language='fr')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"Article vide\")\n",
    "\n",
    "    sentiment, confiance = analyze_overall_sentiment(text)\n",
    "    interpretation = interpret_confiance(confiance)\n",
    "\n",
    "    # R√©sultat\n",
    "    print(\"\\nüîé Analyse de sentiment pour l'article :\")\n",
    "    print(f\"- Sentiment : {sentiment}\")\n",
    "    print(f\"- Score de confiance : {confiance}\")\n",
    "    print(f\"- Interpr√©tation : {interpretation}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Erreur] Impossible d'analyser l'article : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ff1ce-d4e8-4cbf-a677-eac12cc52c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pour compl√©ter le tableau\n",
    "df_sentim.iloc [0,1] = \"Content\"\n",
    "df_sentim.iloc [0,2] = \"0.451\"\n",
    "df_sentim.iloc [0,3] = \"Peu fiable\"\n",
    "\n",
    "df_sentim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010ddb3-892f-4041-8580-9b86eb251818",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Structuration achev√©e\n",
    "\n",
    "df_nouveau['Tendance'] = df_r√©sum['Tendance']\n",
    "df_nouveau['R√©sum√©'] = df_r√©sum['R√©sum√©']\n",
    "df_nouveau['Sentiment_Global'] = df_sentim['Sentiment']\n",
    "\n",
    "df_nouveau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e39aeb-e909-426c-96c6-c6cfda093404",
   "metadata": {},
   "source": [
    "# Pour la r√©alisation du dashboard..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd3463-5869-4c36-a7c0-97da1d867543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df_liens.to_csv('Liens_2025.csv', index=False)\n",
    "df_liens = pd.read_csv ( 'Liens_2025.csv' , sep = '\\t' , encoding = 'latin1' , dtype = 'str' )\n",
    "\n",
    "df_liens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702a0ae-5eb2-40b9-a38c-a50951605775",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pour transformer df_liens en liste\n",
    "URLS = df_liens [ 'Liens' ].tolist()\n",
    "\n",
    "### # Pour la s√©lection des fichiers gkg\n",
    "LINKS = [ url for url in URLS if url.endswith( 'translation.gkg.csv.zip' ) ]\n",
    "for u in LINKS :\n",
    "    print (u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689dda8-f333-45a1-b7f1-d76e1a36913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "OUTPUT_DIR = 'DATA_GKG'    # R√©pertoire de sortie pour les fichiers t√©l√©charg√©s\n",
    "\n",
    "\n",
    "# T√©l√©charger et extraire les fichiers gkg pour une p√©riode donn√©e\n",
    "def download_and_extract(links, output_dir):\n",
    "    for link in tqdm(links[808 : ], desc=\"T√©l√©chargement des fichiers GDELT\"):\n",
    "        file_name = link.split(\"/\")[-1]\n",
    "#        output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # V√©rifier si le fichier existe d√©j√† pour √©viter de le ret√©l√©charger\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(link, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            else:\n",
    "                print(f\"Erreur de t√©l√©chargement pour {link}\")\n",
    "                continue\n",
    "\n",
    "        # D√©compression du fichier ZIP\n",
    "        # V√©rifier si c'est bien un fichier ZIP\n",
    "        if zipfile.is_zipfile(file_name):\n",
    "            print(\"Le fichier est un ZIP valide.\")\n",
    "            with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(output_dir)\n",
    "                print(\"Extraction r√©ussie !\")\n",
    "        else:\n",
    "            print(\"Erreur : Le fichier n'est pas un ZIP valide.\")\n",
    "            continue \n",
    "            \n",
    "        os.remove(file_name)  # Supprimer le fichier ZIP apr√®s extraction pour √©conomiser l'espace\n",
    "\n",
    "\n",
    "download_and_extract( LINKS , OUTPUT_DIR)\n",
    "\n",
    "print ( OUTPUT_DIR )\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088203a8-4537-4bbd-b02b-19eb996f48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_DIR = 'DATA_GKG' \n",
    "\n",
    "\n",
    "\n",
    "# Filtrer les donn√©es par pays\n",
    "def filter_events_by_country(output_dir):\n",
    "    all_data = []\n",
    "    for file in tqdm(os.listdir(output_dir), desc=\"Filtrage des fichiers CSV\"):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        try:\n",
    "            # Lecture des donn√©es CSV en utilisant des chunks pour traiter des fichiers volumineux\n",
    "            for chunk in pd.read_csv(file_path, sep='\\t', chunksize=10, header=None, encoding='latin1', dtype=str):\n",
    "                # La colonne 53 correspond aux √©v√®nements du B√©nin \n",
    "                filtered_chunk = chunk[chunk.iloc[:, 10].str.contains(r\"1#Benin#BN#BN##9.5#2.25#BN#\", na=False)]\n",
    "                all_data.append(filtered_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture de {file}: {e}\")\n",
    "    # Combiner toutes les donn√©es filtr√©es en un seul DataFrame\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "benin_data = filter_events_by_country(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc16f3-79ff-42d5-9d6d-6afd9b045fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATASET = 'Benin_dataset_gkg.csv'  # Nom du fichier final\n",
    "\n",
    "\n",
    "# √âtape 5 : enregistrer le dataset final en csv\n",
    "def save_final_dataset(dataframe, output_file):\n",
    "    dataframe.to_csv(output_file, index=False)\n",
    "    print(f\" Dataset final enregistr√© sous {output_file}\")\n",
    "\n",
    "save_final_dataset(benin_data, FINAL_DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fced7-a162-48b8-9f06-1419656265bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_1 = pd.read_csv( 'Benin_dataset_gkg.csv', sep = ',' , encoding = 'latin1' , dtype = 'str' )\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af8b76-eaab-4f94-9949-3f3b81136a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.columns = [\n",
    "    \"GKGRECORDID\", \"DATE\", \"SourceCollectionIdentifier\", \"SourceCommonName\",\n",
    "    \"DocumentIdentifier\", \"Counts\", \"V2Counts\", \"Themes\", \"V2Themes\", \"Locations\",\n",
    "    \"V2Locations\", \"Persons\", \"V2Persons\", \"Organizations\", \"V2Organizations\",\n",
    "    \"V2Tone\", \"Dates\", \"GCAM\", \"SharingImage\", \"RelatedImages\", \"SocialImageEmbeds\",\n",
    "    \"SocialVideoEmbeds\", \"Quotations\", \"AllNames\", \"Amounts\", \"TranslationInfo\", \"Extras\"\n",
    "]\n",
    "\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d279a5-9334-47aa-8f3f-9a14540058a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame(columns=[\n",
    "    'Identifiant_Unique' , 'Site_Source' , 'Th√®mes' ,\n",
    "    'Lieux' , 'Organisations' \n",
    "])\n",
    "\n",
    "df_2['Identifiant_Unique'] = df_1[\"GKGRECORDID\"]\n",
    "df_2['Site_Source'] = df_1[\"SourceCommonName\"]\n",
    "df_2['Th√®mes'] = df_1[\"V2Themes\"]\n",
    "df_2['Lieux'] = df_1[\"V2Locations\"]\n",
    "df_2['Organisations'] = df_1[\"V2Organizations\"]\n",
    "\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f232e-2a75-463e-a65f-804f69acd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_2['Th√®mes']) )\n",
    "print(df_2['Th√®mes'].str.split().str.len().sum() )\n",
    "print(df_2['Organisations'].str.split().str.len().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76362c8-da52-4c2d-b49c-6efa027cc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5beb3-20dd-4b8d-8834-e4f4c2b78454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261933f9-f98c-4e5f-b4cf-792d644bf0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifi√© s'il y a les lignes r√©p√©t√©es\n",
    "df_2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414c21a-fc10-455e-a862-1065383c55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de valeurs manquantes \n",
    "df_2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9bfd8-0709-4425-96e1-abdc5fbcc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.dropna( inplace=True ) \n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ede10-f5e3-4a3b-8721-e2aa28acfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de valeurs manquantes \n",
    "df_2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0886cd6-c434-4ce0-826d-18ad616bcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv('Th√®mes_Org.csv' , index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d23ac-39fa-45af-920a-5dd23d8c6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ce code sert √† extraire, compter et afficher les th√®mes les plus fr√©quents \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Extraire toutes les listes de th√®mes et les concat√®ne dans une grande liste unique gra√¢ce √† .sum() \n",
    "all_themes = df_2[\"Th√®mes\"].str.split(';').sum()\n",
    "\n",
    "# Compter les occurrences\n",
    "theme_counts = Counter(all_themes)\n",
    "\n",
    "# Cette ligne trie tous les th√®mes par nombre d‚Äôoccurrences, du plus au moins fr√©quent, et stocke le r√©sultat dans une liste de tuples \n",
    "top_themes = theme_counts.most_common()\n",
    "\n",
    "# Affichage\n",
    "for theme, count in top_themes:\n",
    "    if count >= 5 :\n",
    "        print(f\"{theme}: {count}\")    ### Afficher les th√®mes qui sont apparus au moins 5 fois (th√®mes fr√©quents)\n",
    "        \n",
    "# Cr√©er une liste avec les th√®mes ayant au moins 5 occurrences\n",
    "frequent_themes = [theme for theme, count in top_themes if count >= 5]\n",
    "\n",
    "# Affichage de la liste\n",
    "print(frequent_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbc832-c3ae-4c59-adbf-341d427a513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# G√©n√©rer le nuage de mots\n",
    "text = \" \".join(frequent_themes)\n",
    "wordcloud = WordCloud(width=1200, height=800, background_color='white').generate(text)\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Th√®mes dominants dans les articles li√©s au B√©nin\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a863b-5262-401b-b908-54a920861412",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ce code sert √† extraire, compter et afficher les organisations les plus fr√©quentes \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction pour extraire uniquement les noms d‚Äôorganisations (sans les IDs num√©riques)\n",
    "def extract_organization_names(org_series):\n",
    "    org_names = []\n",
    "    for row in org_series :\n",
    "        items = row.split(';')  # S√©parateur entre les organisations\n",
    "        for item in items:\n",
    "            parts = item.split(',')  # S√©paration entre nom et identifiant\n",
    "            if parts:\n",
    "                org_names.append(parts[0].strip())\n",
    "    return org_names\n",
    "\n",
    "# Extraction depuis le DataFrame df_2\n",
    "all_org = extract_organization_names(df_2[\"Organisations\"])\n",
    "\n",
    "# Compter les occurrences\n",
    "org_counts = Counter(all_org)\n",
    "\n",
    "# Cette ligne S√©lectionne le top 15 des organisations les plus fr√©quentes  \n",
    "top_n = 15\n",
    "top_org = org_counts.most_common(top_n)\n",
    "\n",
    "labels = [org for org, count in top_org ]\n",
    "values = [count for org, count in top_org ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5460ff5-2cba-4587-8634-d600e9693406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Diagramme circulaire des organisations les plus fr√©quentes\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=180)\n",
    "plt.title(f\"Top {top_n} des organisations les plus cit√©es dans les articles sur le B√©nin\", fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
